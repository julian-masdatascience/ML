{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearClassification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOodhN+RGB3VBXf4xFKOPNC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julian-masdatascience/ML/blob/master/LinearClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60gEkUu5-Jn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yflJthNF-LUX",
        "colab_type": "text"
      },
      "source": [
        "## Linear Classification\n",
        "### KNN classification\n",
        "In this exercise you'll explore a subset of the [Large Movie Review Datase](http://ai.stanford.edu/~amaas/data/sentiment/)t. The variables X_train, X_test, y_train, and y_test are already loaded into the environment. The X variables contain features based on the words in the movie reviews, and the y variables contain labels for whether the review sentiment is positive (+1) or negative (-1).\n",
        "\n",
        "This course touches on a lot of concepts you may have forgotten, so if you ever need a quick refresher, download the [Scikit-Learn Cheat Sheet](https://datacamp-community-prod.s3.amazonaws.com/5433fa18-9f43-44cc-b228-74672efcd116) and keep it handy!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a KNN model with default hyperparameters.\n",
        "Fit the model.\n",
        "Print out the prediction for the test example 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSoWaHz3AjtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Create and fit the model\n",
        "knn = ____\n",
        "knn.____\n",
        "\n",
        "# Predict on the test features, print the results\n",
        "pred = knn.____[0]\n",
        "print(\"Prediction for test example 0:\", pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1IukGveAkTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Create and fit the model\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the test features, print the results\n",
        "pred = knn.predict(X_test)[0]\n",
        "print(\"Prediction for test example 0:\", pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIvSofioCEss",
        "colab_type": "text"
      },
      "source": [
        "### Comparing models\n",
        "Compare k nearest neighbors classifiers with k=1 and k=5 on the handwritten digits data set, which is already loaded into the variables X_train, y_train, X_test, and y_test. You can set k with the n_neighbors parameter when creating the KNeighborsClassifier object, which is also already imported into the environment.\n",
        "\n",
        "Which model has a higher test accuracy? **5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O51qmgenC6jL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Create and fit the model\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the test features, print the results\n",
        "pred = knn.predict(X_test)[0]\n",
        "print(\"Prediction for test example 0:\", pred)\n",
        "print(knn.score(X_test,y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6QdfnScDBnX",
        "colab_type": "text"
      },
      "source": [
        "### Overfitting\n",
        "Which of the following situations looks like an example of overfitting?\n",
        "\n",
        "Answer the question\n",
        "50 XP\n",
        "Possible Answers\n",
        "Training accuracy 50%, testing accuracy 50%.\n",
        "press\n",
        "1\n",
        "Training accuracy 95%, testing accuracy 95%.\n",
        "press\n",
        "2\n",
        "**Training accuracy 95%, testing accuracy 50%.**\n",
        "press\n",
        "3\n",
        "Training accuracy 50%, testing accuracy 95%.\n",
        "press\n",
        "Por que hay mejores resultados en el entrenamiento que e las pruebas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFgv7ri8DTL9",
        "colab_type": "text"
      },
      "source": [
        "### Running LogisticRegression and SVC\n",
        "In this exercise, you'll apply logistic regression and a support vector machine to classify images of handwritten digits.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Apply logistic regression and SVM (using SVC()) to the handwritten digits data set using the provided train/validation split.\n",
        "For each classifier, print out the training and validation accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev7hCVzNH0CA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets\n",
        "digits = datasets.load_digits()\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
        "\n",
        "# Apply logistic regression and print scores\n",
        "lr = LogisticRegression()  \n",
        "lr.____\n",
        "print(____)\n",
        "print(____)\n",
        "\n",
        "# Apply SVM and print scores\n",
        "svm = ____\n",
        "svm.____\n",
        "print(____)\n",
        "print(____)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGX750L_LD8x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "f574f300-929a-42d6-9ee2-a0bc4fea79f6"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "digits = datasets.load_digits()\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "# Apply logistic regression and print scores\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "print(lr.score(X_train, y_train))\n",
        "print(lr.score(X_test, y_test))\n",
        "\n",
        "# Apply SVM and print scores\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "print(svm.score(X_train, y_train))\n",
        "print(svm.score(X_test, y_test))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.9666666666666667\n",
            "0.9962880475129918\n",
            "0.9933333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX6l0kplN1_2",
        "colab_type": "text"
      },
      "source": [
        "### Sentiment analysis for movie reviews\n",
        "In this exercise you'll explore the probabilities outputted by logistic regression on a subset of the Large Movie Review Dataset.\n",
        "\n",
        "The variables X and y are already loaded into the environment. X contains features based on the number of times words appear in the movie reviews, and y contains labels for whether the review sentiment is positive (+1) or negative (-1).\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Train a logistic regression model on the movie review data.\n",
        "Predict the probabilities of negative vs. positive for the two given reviews.\n",
        "Feel free to write your own reviews and get probabilities for those too!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faW_DhRCH1jr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9d8fd903-cafe-4035-fc86-5ed6ab844166"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import zipfile # para descomprimir archivos zip\n",
        "import urllib.request # para descargar de URL\n",
        "\n",
        "# descargar MovieLens dataset\n",
        "url = 'http://files.grouplens.org/datasets/movielens/ml-1m.zip'  \n",
        "urllib.request.urlretrieve(url, 'm1-1m.zip')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('m1-1m.zip', <http.client.HTTPMessage at 0x7f1452c78da0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e65_ykQbO1Mz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d5ac23cf-da41-4018-b925-271db57117c2"
      },
      "source": [
        "with zipfile.ZipFile('m1-1m.zip', 'r') as zip: \n",
        "    print('Extracting all files...') \n",
        "    zip.extractall('resources/') # destinación\n",
        "    print('Done!') "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting all files...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLDN-Fh_O7EN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "951e06ae-fed5-4880-b78b-339cf85ff3cb"
      },
      "source": [
        "%ls \"resources/ml-1m\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "movies.dat  ratings.dat  README  users.dat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T65mOBPdO8Qa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79b87b10-f904-4a74-ddba-61cbe5ad5337"
      },
      "source": [
        "# descargar MovieLens dataset\n",
        "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'  \n",
        "urllib.request.urlretrieve(url, 'aclImdb_v1.tar.gz')\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('aclImdb_v1.tar.gz', <http.client.HTTPMessage at 0x7f1452c78eb8>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxbgz0rQPZjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "tar = tarfile.open(\"aclImdb_v1.tar.gz\")\n",
        "tar.extractall('sentiment/')\n",
        "tar.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMKvO1XLPiow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6d6f8f8-16f3-4987-ff42-b7c26465bbcf"
      },
      "source": [
        "%ls \"sentiment/aclImdb\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imdbEr.txt  imdb.vocab  README  \u001b[0m\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2Pd4_cZP_BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        " \n",
        "f = np.load('imdb.npz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo9ubO0fQlSd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d9bee5f-3a08-4f30-b034-65c0aba4d8a5"
      },
      "source": [
        "import os\n",
        "print(os.listdir(\"sentiment/aclImdb/train\"))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['unsup', 'unsupBow.feat', 'urls_unsup.txt', 'neg', 'labeledBow.feat', 'urls_neg.txt', 'pos', 'urls_pos.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDj8Rmj7SzSt",
        "colab_type": "text"
      },
      "source": [
        "### Sentiment analysis for movie reviews\n",
        "In this exercise you'll explore the probabilities outputted by logistic regression on a subset of the Large Movie Review Dataset.\n",
        "\n",
        "The variables X and y are already loaded into the environment. X contains features based on the number of times words appear in the movie reviews, and y contains labels for whether the review sentiment is positive (+1) or negative (-1).\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Train a logistic regression model on the movie review data.\n",
        "Predict the probabilities of negative vs. positive for the two given reviews.\n",
        "Feel free to write your own reviews and get probabilities for those too!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veWs4FY7SzcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate logistic regression and train\n",
        "lr = ____\n",
        "lr.fit(____)\n",
        "\n",
        "# Predict sentiment for a glowing review\n",
        "review1 = \"LOVED IT! This movie was amazing. Top 10 this year.\"\n",
        "review1_features = get_features(review1)\n",
        "print(\"Review:\", review1)\n",
        "print(\"Probability of positive review:\", lr.predict_proba(____)[0,1])\n",
        "\n",
        "# Predict sentiment for a poor review\n",
        "review2 = \"Total junk! I'll never watch a film by that director again, no matter how good the reviews.\"\n",
        "review2_features = get_features(review2)\n",
        "print(\"Review:\", review2)\n",
        "print(\"Probability of positive review:\", lr.predict_proba(____)[0,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4ypduvARBVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate logistic regression and train\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X,y)\n",
        "\n",
        "# Predict sentiment for a glowing review\n",
        "review1 = \"LOVED IT! This movie was amazing. Top 10 this year.\"\n",
        "review1_features = get_features(review1)\n",
        "print(\"Review:\", review1)\n",
        "print(\"Probability of positive review:\", lr.predict_proba(review1_features)[0,1])\n",
        "\n",
        "# Predict sentiment for a poor review\n",
        "review2 = \"Total junk! I'll never watch a film by that director again, no matter how good the reviews.\"\n",
        "review2_features = get_features(review2)\n",
        "print(\"Review:\", review2)\n",
        "print(\"Probability of positive review:\", lr.predict_proba(review2_features)[0,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erlm8t02TAVo",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing decision boundaries\n",
        "In this exercise, you'll visualize the decision boundaries of various classifier types.\n",
        "\n",
        "A subset of scikit-learn's built-in wine dataset is already loaded into X, along with binary labels in y.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create the following classifier objects with default hyperparameters: LogisticRegression, LinearSVC, SVC, KNeighborsClassifier.\n",
        "Fit each of the classifiers on the provided data using a for loop.\n",
        "Call the plot_4_classifers() function (similar to the code here), passing in X, y, and a list containing the four classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpZnnXy2TBP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Define the classifiers\n",
        "classifiers = [____]\n",
        "\n",
        "# Fit the classifiers\n",
        "for c in ____:\n",
        "    ____\n",
        "\n",
        "# Plot the classifiers\n",
        "plot_4_classifiers(X, y, classifiers)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pK24Z5hTCT7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "3890bf6f-e339-4e58-e497-58099151be7d"
      },
      "source": [
        "import sklearn.datasets \n",
        "wine = sklearn.datasets.load_wine()\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# Define the classifiers\n",
        "\n",
        "X = wine.data\n",
        "y= wine.target\n",
        "\n",
        "# Define the classifiers\n",
        "classifiers = [LogisticRegression(),LinearSVC(),SVC(),KNeighborsClassifier()]\n",
        "\n",
        "# Fit the classifiers\n",
        "for c in classifiers:\n",
        "    c.fit(X,y)\n",
        "\n",
        "# Plot the classifiers\n",
        "plot_4_classifiers(X, y, classifiers)\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-1df21010f544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Plot the classifiers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mplot_4_classifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_4_classifiers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVH1geWk85y8",
        "colab_type": "text"
      },
      "source": [
        "## Linear classifiers:prediction equations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NugzD_mp86ax",
        "colab_type": "text"
      },
      "source": [
        "### How models make predictions\n",
        "Which classifiers make predictions based on the sign (positive or negative) of the raw model output?\n",
        "\n",
        "Answer the question\n",
        "50 XP\n",
        "Possible Answers\n",
        "Logistic regression only\n",
        "press\n",
        "1\n",
        "Linear SVMs only\n",
        "press\n",
        "2\n",
        "Neither\n",
        "press\n",
        "3\n",
        "Both logistic regression and Linear SVMs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_uF0jBo9FHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the coefficients\n",
        "model.coef_ = np.array([[0,1]])\n",
        "model.intercept_ = np.array([0])\n",
        "\n",
        "# Plot the data and decision boundary\n",
        "plot_classifier(X,y,model)\n",
        "\n",
        "# Print the number of errors\n",
        "num_err = np.sum(y != model.predict(X))\n",
        "print(\"Number of errors:\", num_err)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzEtNPtH9mQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the coefficients\n",
        "model.coef_ = np.array([[0,1]])\n",
        "model.intercept_ = np.array([0])\n",
        "\n",
        "# Plot the data and decision boundary\n",
        "plot_classifier(X,y,model)\n",
        "\n",
        "# Print the number of errors\n",
        "num_err = np.sum(y != model.predict(X))\n",
        "print(\"Number of errors:\", num_err)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY_bxBxo_u5B",
        "colab_type": "text"
      },
      "source": [
        "### Minimizing a loss function\n",
        "In this exercise you'll implement linear regression \"from scratch\" using scipy.optimize.minimize.\n",
        "\n",
        "We'll train a model on the Boston housing price data set, which is already loaded into the variables X and y. For simplicity, we won't include an intercept in our regression model.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Fill in the loss function for least squares linear regression.\n",
        "Print out the coefficients from fitting sklearn's LinearRegression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRS-S5Pb_3fX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The squared error, summed over training examples\n",
        "def my_loss(w):\n",
        "    s = 0\n",
        "    for i in range(y.size):\n",
        "        # Get the true and predicted target values for example 'i'\n",
        "        y_i_true = y[i]\n",
        "        y_i_pred = w@X[i]\n",
        "        s = s + (____)**2\n",
        "    return s\n",
        "\n",
        "# Returns the w that makes my_loss(w) smallest\n",
        "w_fit = minimize(my_loss, X[0]).x\n",
        "print(w_fit)\n",
        "\n",
        "# Compare with scikit-learn's LinearRegression coefficients\n",
        "lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
        "print(____)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqNrCKyF_5SD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b2a6fb29-8b75-4bf2-cbc4-1d4b57c9316d"
      },
      "source": [
        "def my_loss(w):\n",
        "    s = 0\n",
        "    for i in range(y.size):\n",
        "        # Get the true and predicted target values for example 'i'\n",
        "        y_i_true = y[i]\n",
        "        y_i_pred = w@X[i]\n",
        "        s = s + (y_i_true - y_i_pred)**2\n",
        "    return s\n",
        "\n",
        "# Returns the w that makes my_loss(w) smallest\n",
        "w_fit = minimize(my_loss, X[0]).x\n",
        "print(w_fit)\n",
        "\n",
        "# Compare with scikit-learn's LinearRegression coefficients\n",
        "lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
        "print(lr.coef_)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7466abdc22fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Returns the w that makes my_loss(w) smallest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mw_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_fit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'minimize' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7UDFS_3K3Oo",
        "colab_type": "text"
      },
      "source": [
        "### Comparing the logistic and hinge losses\n",
        "In this exercise you'll create a plot of the logistic and hinge losses using their mathematical expressions, which are provided to you.\n",
        "\n",
        "The loss function diagram from the video is shown on the right.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Evaluate the log_loss() and hinge_loss() functions at the grid points so that they are plotted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOVUnja-KyHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mathematical functions for logistic and hinge losses\n",
        "def log_loss(raw_model_output):\n",
        "   return np.log(1+np.exp(-raw_model_output))\n",
        "def hinge_loss(raw_model_output):\n",
        "   return np.maximum(0,1-raw_model_output)\n",
        "\n",
        "# Create a grid of values and plot\n",
        "grid = np.linspace(-2,2,1000)\n",
        "plt.plot(grid, ____, label='logistic')\n",
        "plt.plot(grid, ____, label='hinge')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw49ViXCKyNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mathematical functions for logistic and hinge losses\n",
        "def log_loss(raw_model_output):\n",
        "   return np.log(1+np.exp(-raw_model_output))\n",
        "def hinge_loss(raw_model_output):\n",
        "   return np.maximum(0,1-raw_model_output)\n",
        "\n",
        "# Create a grid of values and plot\n",
        "grid = np.linspace(-2,2,1000)\n",
        "plt.plot(grid, log_loss(grid), label='logistic')\n",
        "plt.plot(grid, hinge_loss(grid), label='hinge')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2HzWTbkBgPd",
        "colab_type": "text"
      },
      "source": [
        "### Implementing logistic regression\n",
        "This is very similar to the earlier exercise where you implemented linear regression \"from scratch\" using scipy.optimize.minimize. However, this time we'll minimize the logistic loss and compare with scikit-learn's LogisticRegression (we've set C to a large value to disable regularization; more on this in Chapter 3!).\n",
        "\n",
        "The log_loss() function from the previous exercise is already defined in your environment, and the sklearn breast cancer prediction dataset (first 10 features, standardized) is loaded into the variables X and y.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Input the number of training examples into range().\n",
        "Fill in the loss function for logistic regression.\n",
        "Compare the coefficients to sklearn's LogisticRegression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMzyxIk4LHEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The logistic loss, summed over training examples\n",
        "def my_loss(w):\n",
        "    s = 0\n",
        "    for i in range(____):\n",
        "        raw_model_output = w@X[i]\n",
        "        s = s + ____(raw_model_output * y[i])\n",
        "    return s\n",
        "\n",
        "# Returns the w that makes my_loss(w) smallest\n",
        "w_fit = minimize(my_loss, X[0]).x\n",
        "print(w_fit)\n",
        "\n",
        "# Compare with scikit-learn's LogisticRegression\n",
        "lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y)\n",
        "print(lr.coef_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkKdZ1k1LLWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The logistic loss, summed over training examples\n",
        "def my_loss(w):\n",
        "    s = 0\n",
        "    for i in range(0, 569):\n",
        "        raw_model_output = w@X[i]\n",
        "        s = s + log_loss(raw_model_output * y[i])\n",
        "    return s\n",
        "\n",
        "# Returns the w that makes my_loss(w) smallest\n",
        "w_fit = minimize(my_loss, X[0]).x\n",
        "print(w_fit)\n",
        "\n",
        "# Compare with scikit-learn's LogisticRegression\n",
        "lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y)\n",
        "print(lr.coef_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmVun33FLgfs",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}