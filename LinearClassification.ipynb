{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearClassification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNInD0L8LzKI8LcoHystyXf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julian-masdatascience/ML/blob/master/LinearClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60gEkUu5-Jn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yflJthNF-LUX",
        "colab_type": "text"
      },
      "source": [
        "## Linear Classification\n",
        "### KNN classification\n",
        "In this exercise you'll explore a subset of the [Large Movie Review Datase](http://ai.stanford.edu/~amaas/data/sentiment/)t. The variables X_train, X_test, y_train, and y_test are already loaded into the environment. The X variables contain features based on the words in the movie reviews, and the y variables contain labels for whether the review sentiment is positive (+1) or negative (-1).\n",
        "\n",
        "This course touches on a lot of concepts you may have forgotten, so if you ever need a quick refresher, download the [Scikit-Learn Cheat Sheet](https://datacamp-community-prod.s3.amazonaws.com/5433fa18-9f43-44cc-b228-74672efcd116) and keep it handy!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a KNN model with default hyperparameters.\n",
        "Fit the model.\n",
        "Print out the prediction for the test example 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSoWaHz3AjtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Create and fit the model\n",
        "knn = ____\n",
        "knn.____\n",
        "\n",
        "# Predict on the test features, print the results\n",
        "pred = knn.____[0]\n",
        "print(\"Prediction for test example 0:\", pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1IukGveAkTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Create and fit the model\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the test features, print the results\n",
        "pred = knn.predict(X_test)[0]\n",
        "print(\"Prediction for test example 0:\", pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIvSofioCEss",
        "colab_type": "text"
      },
      "source": [
        "### Comparing models\n",
        "Compare k nearest neighbors classifiers with k=1 and k=5 on the handwritten digits data set, which is already loaded into the variables X_train, y_train, X_test, and y_test. You can set k with the n_neighbors parameter when creating the KNeighborsClassifier object, which is also already imported into the environment.\n",
        "\n",
        "Which model has a higher test accuracy? **5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O51qmgenC6jL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Create and fit the model\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the test features, print the results\n",
        "pred = knn.predict(X_test)[0]\n",
        "print(\"Prediction for test example 0:\", pred)\n",
        "print(knn.score(X_test,y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6QdfnScDBnX",
        "colab_type": "text"
      },
      "source": [
        "### Overfitting\n",
        "Which of the following situations looks like an example of overfitting?\n",
        "\n",
        "Answer the question\n",
        "50 XP\n",
        "Possible Answers\n",
        "Training accuracy 50%, testing accuracy 50%.\n",
        "press\n",
        "1\n",
        "Training accuracy 95%, testing accuracy 95%.\n",
        "press\n",
        "2\n",
        "**Training accuracy 95%, testing accuracy 50%.**\n",
        "press\n",
        "3\n",
        "Training accuracy 50%, testing accuracy 95%.\n",
        "press\n",
        "Por que hay mejores resultados en el entrenamiento que e las pruebas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFgv7ri8DTL9",
        "colab_type": "text"
      },
      "source": [
        "### Running LogisticRegression and SVC\n",
        "In this exercise, you'll apply logistic regression and a support vector machine to classify images of handwritten digits.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Apply logistic regression and SVM (using SVC()) to the handwritten digits data set using the provided train/validation split.\n",
        "For each classifier, print out the training and validation accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev7hCVzNH0CA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets\n",
        "digits = datasets.load_digits()\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
        "\n",
        "# Apply logistic regression and print scores\n",
        "lr = LogisticRegression()  \n",
        "lr.____\n",
        "print(____)\n",
        "print(____)\n",
        "\n",
        "# Apply SVM and print scores\n",
        "svm = ____\n",
        "svm.____\n",
        "print(____)\n",
        "print(____)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGX750L_LD8x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "f574f300-929a-42d6-9ee2-a0bc4fea79f6"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "digits = datasets.load_digits()\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "# Apply logistic regression and print scores\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "print(lr.score(X_train, y_train))\n",
        "print(lr.score(X_test, y_test))\n",
        "\n",
        "# Apply SVM and print scores\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "print(svm.score(X_train, y_train))\n",
        "print(svm.score(X_test, y_test))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.9666666666666667\n",
            "0.9962880475129918\n",
            "0.9933333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX6l0kplN1_2",
        "colab_type": "text"
      },
      "source": [
        "### Sentiment analysis for movie reviews\n",
        "In this exercise you'll explore the probabilities outputted by logistic regression on a subset of the Large Movie Review Dataset.\n",
        "\n",
        "The variables X and y are already loaded into the environment. X contains features based on the number of times words appear in the movie reviews, and y contains labels for whether the review sentiment is positive (+1) or negative (-1).\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Train a logistic regression model on the movie review data.\n",
        "Predict the probabilities of negative vs. positive for the two given reviews.\n",
        "Feel free to write your own reviews and get probabilities for those too!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faW_DhRCH1jr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9d8fd903-cafe-4035-fc86-5ed6ab844166"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import zipfile # para descomprimir archivos zip\n",
        "import urllib.request # para descargar de URL\n",
        "\n",
        "# descargar MovieLens dataset\n",
        "url = 'http://files.grouplens.org/datasets/movielens/ml-1m.zip'  \n",
        "urllib.request.urlretrieve(url, 'm1-1m.zip')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('m1-1m.zip', <http.client.HTTPMessage at 0x7f1452c78da0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e65_ykQbO1Mz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d5ac23cf-da41-4018-b925-271db57117c2"
      },
      "source": [
        "with zipfile.ZipFile('m1-1m.zip', 'r') as zip: \n",
        "    print('Extracting all files...') \n",
        "    zip.extractall('resources/') # destinación\n",
        "    print('Done!') "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting all files...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLDN-Fh_O7EN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "951e06ae-fed5-4880-b78b-339cf85ff3cb"
      },
      "source": [
        "%ls \"resources/ml-1m\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "movies.dat  ratings.dat  README  users.dat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T65mOBPdO8Qa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79b87b10-f904-4a74-ddba-61cbe5ad5337"
      },
      "source": [
        "# descargar MovieLens dataset\n",
        "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'  \n",
        "urllib.request.urlretrieve(url, 'aclImdb_v1.tar.gz')\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('aclImdb_v1.tar.gz', <http.client.HTTPMessage at 0x7f1452c78eb8>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxbgz0rQPZjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "tar = tarfile.open(\"aclImdb_v1.tar.gz\")\n",
        "tar.extractall('sentiment/')\n",
        "tar.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMKvO1XLPiow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6d6f8f8-16f3-4987-ff42-b7c26465bbcf"
      },
      "source": [
        "%ls \"sentiment/aclImdb\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imdbEr.txt  imdb.vocab  README  \u001b[0m\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2Pd4_cZP_BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        " \n",
        "f = np.load('imdb.npz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo9ubO0fQlSd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d9bee5f-3a08-4f30-b034-65c0aba4d8a5"
      },
      "source": [
        "import os\n",
        "print(os.listdir(\"sentiment/aclImdb/train\"))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['unsup', 'unsupBow.feat', 'urls_unsup.txt', 'neg', 'labeledBow.feat', 'urls_neg.txt', 'pos', 'urls_pos.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDj8Rmj7SzSt",
        "colab_type": "text"
      },
      "source": [
        "### Sentiment analysis for movie reviews\n",
        "In this exercise you'll explore the probabilities outputted by logistic regression on a subset of the Large Movie Review Dataset.\n",
        "\n",
        "The variables X and y are already loaded into the environment. X contains features based on the number of times words appear in the movie reviews, and y contains labels for whether the review sentiment is positive (+1) or negative (-1).\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Train a logistic regression model on the movie review data.\n",
        "Predict the probabilities of negative vs. positive for the two given reviews.\n",
        "Feel free to write your own reviews and get probabilities for those too!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veWs4FY7SzcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate logistic regression and train\n",
        "lr = ____\n",
        "lr.fit(____)\n",
        "\n",
        "# Predict sentiment for a glowing review\n",
        "review1 = \"LOVED IT! This movie was amazing. Top 10 this year.\"\n",
        "review1_features = get_features(review1)\n",
        "print(\"Review:\", review1)\n",
        "print(\"Probability of positive review:\", lr.predict_proba(____)[0,1])\n",
        "\n",
        "# Predict sentiment for a poor review\n",
        "review2 = \"Total junk! I'll never watch a film by that director again, no matter how good the reviews.\"\n",
        "review2_features = get_features(review2)\n",
        "print(\"Review:\", review2)\n",
        "print(\"Probability of positive review:\", lr.predict_proba(____)[0,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4ypduvARBVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate logistic regression and train\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X,y)\n",
        "\n",
        "# Predict sentiment for a glowing review\n",
        "review1 = \"LOVED IT! This movie was amazing. Top 10 this year.\"\n",
        "review1_features = get_features(review1)\n",
        "print(\"Review:\", review1)\n",
        "print(\"Probability of positive review:\", lr.predict_proba(review1_features)[0,1])\n",
        "\n",
        "# Predict sentiment for a poor review\n",
        "review2 = \"Total junk! I'll never watch a film by that director again, no matter how good the reviews.\"\n",
        "review2_features = get_features(review2)\n",
        "print(\"Review:\", review2)\n",
        "print(\"Probability of positive review:\", lr.predict_proba(review2_features)[0,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erlm8t02TAVo",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing decision boundaries\n",
        "In this exercise, you'll visualize the decision boundaries of various classifier types.\n",
        "\n",
        "A subset of scikit-learn's built-in wine dataset is already loaded into X, along with binary labels in y.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create the following classifier objects with default hyperparameters: LogisticRegression, LinearSVC, SVC, KNeighborsClassifier.\n",
        "Fit each of the classifiers on the provided data using a for loop.\n",
        "Call the plot_4_classifers() function (similar to the code here), passing in X, y, and a list containing the four classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpZnnXy2TBP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Define the classifiers\n",
        "classifiers = [____]\n",
        "\n",
        "# Fit the classifiers\n",
        "for c in ____:\n",
        "    ____\n",
        "\n",
        "# Plot the classifiers\n",
        "plot_4_classifiers(X, y, classifiers)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pK24Z5hTCT7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "3890bf6f-e339-4e58-e497-58099151be7d"
      },
      "source": [
        "import sklearn.datasets \n",
        "wine = sklearn.datasets.load_wine()\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# Define the classifiers\n",
        "\n",
        "X = wine.data\n",
        "y= wine.target\n",
        "\n",
        "# Define the classifiers\n",
        "classifiers = [LogisticRegression(),LinearSVC(),SVC(),KNeighborsClassifier()]\n",
        "\n",
        "# Fit the classifiers\n",
        "for c in classifiers:\n",
        "    c.fit(X,y)\n",
        "\n",
        "# Plot the classifiers\n",
        "plot_4_classifiers(X, y, classifiers)\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-1df21010f544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Plot the classifiers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mplot_4_classifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_4_classifiers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVH1geWk85y8",
        "colab_type": "text"
      },
      "source": [
        "## Linear classifiers:prediction equations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NugzD_mp86ax",
        "colab_type": "text"
      },
      "source": [
        "### How models make predictions\n",
        "Which classifiers make predictions based on the sign (positive or negative) of the raw model output?\n",
        "\n",
        "Answer the question\n",
        "50 XP\n",
        "Possible Answers\n",
        "Logistic regression only\n",
        "press\n",
        "1\n",
        "Linear SVMs only\n",
        "press\n",
        "2\n",
        "Neither\n",
        "press\n",
        "3\n",
        "Both logistic regression and Linear SVMs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_uF0jBo9FHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the coefficients\n",
        "model.coef_ = np.array([[0,1]])\n",
        "model.intercept_ = np.array([0])\n",
        "\n",
        "# Plot the data and decision boundary\n",
        "plot_classifier(X,y,model)\n",
        "\n",
        "# Print the number of errors\n",
        "num_err = np.sum(y != model.predict(X))\n",
        "print(\"Number of errors:\", num_err)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzEtNPtH9mQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the coefficients\n",
        "model.coef_ = np.array([[0,1]])\n",
        "model.intercept_ = np.array([0])\n",
        "\n",
        "# Plot the data and decision boundary\n",
        "plot_classifier(X,y,model)\n",
        "\n",
        "# Print the number of errors\n",
        "num_err = np.sum(y != model.predict(X))\n",
        "print(\"Number of errors:\", num_err)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY_bxBxo_u5B",
        "colab_type": "text"
      },
      "source": [
        "### Minimizing a loss function\n",
        "In this exercise you'll implement linear regression \"from scratch\" using scipy.optimize.minimize.\n",
        "\n",
        "We'll train a model on the Boston housing price data set, which is already loaded into the variables X and y. For simplicity, we won't include an intercept in our regression model.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Fill in the loss function for least squares linear regression.\n",
        "Print out the coefficients from fitting sklearn's LinearRegression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRS-S5Pb_3fX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The squared error, summed over training examples\n",
        "def my_loss(w):\n",
        "    s = 0\n",
        "    for i in range(y.size):\n",
        "        # Get the true and predicted target values for example 'i'\n",
        "        y_i_true = y[i]\n",
        "        y_i_pred = w@X[i]\n",
        "        s = s + (____)**2\n",
        "    return s\n",
        "\n",
        "# Returns the w that makes my_loss(w) smallest\n",
        "w_fit = minimize(my_loss, X[0]).x\n",
        "print(w_fit)\n",
        "\n",
        "# Compare with scikit-learn's LinearRegression coefficients\n",
        "lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
        "print(____)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqNrCKyF_5SD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b2a6fb29-8b75-4bf2-cbc4-1d4b57c9316d"
      },
      "source": [
        "def my_loss(w):\n",
        "    s = 0\n",
        "    for i in range(y.size):\n",
        "        # Get the true and predicted target values for example 'i'\n",
        "        y_i_true = y[i]\n",
        "        y_i_pred = w@X[i]\n",
        "        s = s + (y_i_true - y_i_pred)**2\n",
        "    return s\n",
        "\n",
        "# Returns the w that makes my_loss(w) smallest\n",
        "w_fit = minimize(my_loss, X[0]).x\n",
        "print(w_fit)\n",
        "\n",
        "# Compare with scikit-learn's LinearRegression coefficients\n",
        "lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
        "print(lr.coef_)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7466abdc22fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Returns the w that makes my_loss(w) smallest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mw_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_fit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'minimize' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7UDFS_3K3Oo",
        "colab_type": "text"
      },
      "source": [
        "### Comparing the logistic and hinge losses\n",
        "In this exercise you'll create a plot of the logistic and hinge losses using their mathematical expressions, which are provided to you.\n",
        "\n",
        "The loss function diagram from the video is shown on the right.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Evaluate the log_loss() and hinge_loss() functions at the grid points so that they are plotted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOVUnja-KyHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mathematical functions for logistic and hinge losses\n",
        "def log_loss(raw_model_output):\n",
        "   return np.log(1+np.exp(-raw_model_output))\n",
        "def hinge_loss(raw_model_output):\n",
        "   return np.maximum(0,1-raw_model_output)\n",
        "\n",
        "# Create a grid of values and plot\n",
        "grid = np.linspace(-2,2,1000)\n",
        "plt.plot(grid, ____, label='logistic')\n",
        "plt.plot(grid, ____, label='hinge')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw49ViXCKyNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mathematical functions for logistic and hinge losses\n",
        "def log_loss(raw_model_output):\n",
        "   return np.log(1+np.exp(-raw_model_output))\n",
        "def hinge_loss(raw_model_output):\n",
        "   return np.maximum(0,1-raw_model_output)\n",
        "\n",
        "# Create a grid of values and plot\n",
        "grid = np.linspace(-2,2,1000)\n",
        "plt.plot(grid, log_loss(grid), label='logistic')\n",
        "plt.plot(grid, hinge_loss(grid), label='hinge')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2HzWTbkBgPd",
        "colab_type": "text"
      },
      "source": [
        "### Implementing logistic regression\n",
        "This is very similar to the earlier exercise where you implemented linear regression \"from scratch\" using scipy.optimize.minimize. However, this time we'll minimize the logistic loss and compare with scikit-learn's LogisticRegression (we've set C to a large value to disable regularization; more on this in Chapter 3!).\n",
        "\n",
        "The log_loss() function from the previous exercise is already defined in your environment, and the sklearn breast cancer prediction dataset (first 10 features, standardized) is loaded into the variables X and y.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Input the number of training examples into range().\n",
        "Fill in the loss function for logistic regression.\n",
        "Compare the coefficients to sklearn's LogisticRegression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMzyxIk4LHEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The logistic loss, summed over training examples\n",
        "def my_loss(w):\n",
        "    s = 0\n",
        "    for i in range(____):\n",
        "        raw_model_output = w@X[i]\n",
        "        s = s + ____(raw_model_output * y[i])\n",
        "    return s\n",
        "\n",
        "# Returns the w that makes my_loss(w) smallest\n",
        "w_fit = minimize(my_loss, X[0]).x\n",
        "print(w_fit)\n",
        "\n",
        "# Compare with scikit-learn's LogisticRegression\n",
        "lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y)\n",
        "print(lr.coef_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkKdZ1k1LLWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The logistic loss, summed over training examples\n",
        "def my_loss(w):\n",
        "    s = 0\n",
        "    for i in range(0, 569):\n",
        "        raw_model_output = w@X[i]\n",
        "        s = s + log_loss(raw_model_output * y[i])\n",
        "    return s\n",
        "\n",
        "# Returns the w that makes my_loss(w) smallest\n",
        "w_fit = minimize(my_loss, X[0]).x\n",
        "print(w_fit)\n",
        "\n",
        "# Compare with scikit-learn's LogisticRegression\n",
        "lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y)\n",
        "print(lr.coef_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmVun33FLgfs",
        "colab_type": "text"
      },
      "source": [
        "### Regularized logistic regression\n",
        "In Chapter 1, you used logistic regression on the handwritten digits data set. Here, we'll explore the effect of L2 regularization.\n",
        "\n",
        "The handwritten digits dataset is already loaded, split, and stored in the variables X_train, y_train, X_valid, and y_valid. The variables train_errs and valid_errs are already initialized as empty lists.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Loop over the different values of C_value, creating and fitting a LogisticRegression model each time.\n",
        "Save the error on the training set and the validation set for each model.\n",
        "Create a plot of the training and testing error as a function of the regularization parameter, C.\n",
        "Looking at the plot, what's the best value of C?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0kV8EwvZqAg",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkpCkzPaZq2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train and validaton errors initialized as empty list\n",
        "train_errs = list()\n",
        "valid_errs = list()\n",
        "\n",
        "# Loop over values of C_value\n",
        "for C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
        "    # Create LogisticRegression object and fit\n",
        "    lr = ____\n",
        "    lr.fit(____)\n",
        "    \n",
        "    # Evaluate error rates and append to lists\n",
        "    train_errs.append( 1.0 - lr.score(____) )\n",
        "    valid_errs.append( 1.0 - lr.score(____) )\n",
        "    \n",
        "# Plot results\n",
        "plt.semilogx(C_values, train_errs, C_values, valid_errs)\n",
        "plt.legend((\"train\", \"validation\"))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHlwiVhHZrdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train and validaton errors initialized as empty list\n",
        "train_errs = list()\n",
        "valid_errs = list()\n",
        "\n",
        "# Loop over values of C_value\n",
        "for C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
        "    # Create LogisticRegression object and fit\n",
        "    lr = LogisticRegression(C=C_value)\n",
        "    lr.fit(X_train,y_train)\n",
        "    y_predict = lr.predict(X_valid)\n",
        "    # Evaluate error rates and append to lists\n",
        "    train_errs.append( 1.0 - lr.score(X_train,y_train) )\n",
        "    valid_errs.append( 1.0 - lr.score(X_valid,y_valid) )\n",
        "    \n",
        "# Plot results\n",
        "plt.semilogx(C_values, train_errs, C_values, valid_errs)\n",
        "plt.legend((\"train\", \"validation\"))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxixl8yIcHQE",
        "colab_type": "text"
      },
      "source": [
        "Congrats! As you can see, too much regularization (small C) doesn't work well - due to underfitting - and too little regularization (large C) doesn't work well either - due to overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2uRiPxucIEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H0VI4rucITx",
        "colab_type": "text"
      },
      "source": [
        "### Logistic regression and feature selection\n",
        "In this exercise we'll perform feature selection on the movie review sentiment data set using L1 regularization. The features and targets are already loaded for you in X_train and y_train.\n",
        "\n",
        "We'll search for the best value of C using scikit-learn's GridSearchCV(), which was covered in the prerequisite course.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Instantiate a logistic regression object that uses L1 regularization.\n",
        "Find the value of C that minimizes cross-validation error.\n",
        "Print out the number of selected features for this value of C.\n",
        "Take Hint (-30 XP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0MDxm7acK9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify L1 regularization\n",
        "lr = LogisticRegression(____)\n",
        "\n",
        "# Instantiate the GridSearchCV object and run the search\n",
        "searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\n",
        "searcher.fit(X_train, y_train)\n",
        "\n",
        "# Report the best parameters\n",
        "print(\"Best CV params\", searcher.best_params_)\n",
        "\n",
        "# Find the number of nonzero coefficients (selected features)\n",
        "best_lr = searcher.best_estimator_\n",
        "coefs = best_lr.____\n",
        "print(\"Total number of features:\", coefs.size)\n",
        "print(\"Number of selected features:\", np.count_nonzero(coefs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us_lUapucdul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify L1 regularization\n",
        "lr = LogisticRegression(penalty='l1')\n",
        "\n",
        "# Instantiate the GridSearchCV object and run the search\n",
        "searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\n",
        "searcher.fit(X_train, y_train)\n",
        "\n",
        "# Report the best parameters\n",
        "print(\"Best CV params\", searcher.best_params_)\n",
        "\n",
        "# Find the number of nonzero coefficients (selected features)\n",
        "best_lr = searcher.best_estimator_\n",
        "coefs = best_lr.coef_\n",
        "print(\"Total number of features:\", coefs.size)\n",
        "print(\"Number of selected features:\", np.count_nonzero(coefs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFdlAaz3dO0P",
        "colab_type": "text"
      },
      "source": [
        "### Identifying the most positive and negative words\n",
        "In this exercise we'll try to interpret the coefficients of a logistic regression fit on the movie review sentiment dataset. The model object is already instantiated and fit for you in the variable lr.\n",
        "\n",
        "In addition, the words corresponding to the different features are loaded into the variable vocab. For example, since vocab[100] is \"think\", that means feature 100 corresponds to the number of times the word \"think\" appeared in that movie review.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Find the words corresponding to the 5 largest coefficients.\n",
        "Find the words corresponding to the 5 smallest coefficients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcinVGfYdTAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the indices of the sorted cofficients\n",
        "inds_ascending = np.argsort(lr.coef_.flatten()) \n",
        "inds_descending = inds_ascending[::-1]\n",
        "\n",
        "# Print the most positive words\n",
        "print(\"Most positive words: \", end=\"\")\n",
        "for i in range(5):\n",
        "    print(____, end=\", \")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Print most negative words\n",
        "print(\"Most negative words: \", end=\"\")\n",
        "for i in range(5):\n",
        "    print(____, end=\", \")\n",
        "print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7Fx9L9adtsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the indices of the sorted cofficients\n",
        "inds_ascending = np.argsort(lr.coef_.flatten()) \n",
        "inds_descending = inds_ascending[::-1]\n",
        "\n",
        "# Print the most positive words\n",
        "print(\"Most positive words: \", end=\"\")\n",
        "for i in range(5):\n",
        "    print(vocab[inds_descending[i]], end=\", \")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Print most negative words\n",
        "print(\"Most negative words: \", end=\"\")\n",
        "for i in range(5):\n",
        "    print(vocab[inds_ascending[i]], end=\", \")\n",
        "print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX13dmpAgVn0",
        "colab_type": "text"
      },
      "source": [
        "## Logistict regresion and probabilities\n",
        "### Regularization and probabilities\n",
        "In this exercise, you will observe the effects of changing the regularization strength on the predicted probabilities.\n",
        "\n",
        "A 2D binary classification dataset is already loaded into the environment as X and y.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Compute the maximum predicted probability.\n",
        "Run the provided code and take a look at the plot.\n",
        "Take Hint (-15 XP)\n",
        "2\n",
        "Create a model with C=0.1 and examine how the plot and probabilities change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rFVHwl4gZ41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the regularization strength\n",
        "model = LogisticRegression(C=1)\n",
        "\n",
        "# Fit and plot\n",
        "model.fit(X,y)\n",
        "plot_classifier(X,y,model,proba=True)\n",
        "\n",
        "# Predict probabilities on training points\n",
        "prob = model.predict_proba(X)\n",
        "print(\"Maximum predicted probability\", ____)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cxRl_MKgbTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LogisticRegression(C=0.1)\n",
        "\n",
        "# Fit and plot\n",
        "model.fit(X,y)\n",
        "plot_classifier(X,y,model,proba=True)\n",
        "\n",
        "# Predict probabilities on training points\n",
        "prob = model.predict_proba(X)\n",
        "print(\"Maximum predicted probability\", np.max(prob))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BETDkfg3hNBZ",
        "colab_type": "text"
      },
      "source": [
        " smaller values of C lead to less confident predictions. That's because smaller C means more regularization, which in turn means smaller coefficients, which means raw model outputs closer to zero and, thus, probabilities closer to 0.5 after the raw model output is squashed through the sigmoid function. That's quite a chain of events!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2_jbzmMhRNv",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing easy and difficult examples\n",
        "In this exercise, you'll visualize the examples that the logistic regression model is most and least confident about by looking at the largest and smallest predicted probabilities.\n",
        "\n",
        "The handwritten digits dataset is already loaded into the variables X and y. The show_digit function takes in an integer index and plots the corresponding image, with some extra information displayed above the image.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Fill in the first blank with the index of the digit that the model is most confident about.\n",
        "Fill in the second blank with the index of the digit that the model is least confident about.\n",
        "Observe the images: do you agree that the first one is less ambiguous than the second?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cfsR3wWhSnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(X,y)\n",
        "\n",
        "# Get predicted probabilities\n",
        "proba = lr.predict_proba(X)\n",
        "\n",
        "# Sort the example indices by their maximum probability\n",
        "proba_inds = np.argsort(np.max(proba,axis=1))\n",
        "\n",
        "# Show the most confident (least ambiguous) digit\n",
        "show_digit(____, lr)\n",
        "\n",
        "# Show the least confident (most ambiguous) digit\n",
        "show_digit(____, lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmBy7GT1hUDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(X,y)\n",
        "\n",
        "# Get predicted probabilities\n",
        "proba = lr.predict_proba(X)\n",
        "\n",
        "# Sort the example indices by their maximum probability\n",
        "proba_inds = np.argsort(np.max(proba,axis=1))\n",
        "\n",
        "# Show the most confident (least ambiguous) digit\n",
        "show_digit(proba_inds[-1], lr)\n",
        "\n",
        "\n",
        "# Show the least confident (most ambiguous) digit\n",
        "show_digit(proba_inds[0], lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aJH3Opoh-7V",
        "colab_type": "text"
      },
      "source": [
        "## Multi-class logistic regression\n",
        "\n",
        "### question : Counting the coefficients\n",
        "If you fit a logistic regression model on a classification problem with 3 classes and 100 features, how many coefficients would you have, including intercepts? \n",
        "Possible Answers\n",
        "101\n",
        "press\n",
        "1\n",
        "103\n",
        "press\n",
        "2\n",
        "301\n",
        "press\n",
        "3\n",
        "**303**\n",
        "press\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkUyaHnWmot3",
        "colab_type": "text"
      },
      "source": [
        "### Fitting multi-class logistic regression\n",
        "In this exercise, you'll fit the two types of multi-class logistic regression, one-vs-rest and softmax/multinomial, on the handwritten digits data set and compare the results. The handwritten digits dataset is already loaded and split into X_train, y_train, X_test, and y_test.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Fit a one-vs-rest logistic regression classifier and report the results.\n",
        "Fit a multinomial logistic regression classifier by setting the multi_class parameter, plus setting to be solver = \"lbfgs\", and report the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PRxwjf7mjgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit one-vs-rest logistic regression classifier\n",
        "lr_ovr = ____\n",
        "lr_ovr.fit(X_train, y_train)\n",
        "\n",
        "print(\"OVR training accuracy:\", lr_ovr.score(X_train, y_train))\n",
        "print(\"OVR test accuracy    :\", lr_ovr.score(X_test, y_test))\n",
        "\n",
        "# Fit softmax classifier\n",
        "lr_mn = ____\n",
        "lr_mn.fit(X_train, y_train)\n",
        "\n",
        "print(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\n",
        "print(\"Softmax test accuracy    :\", lr_mn.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MlovVEenHNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit one-vs-rest logistic regression classifier\n",
        "lr_ovr = LogisticRegression()\n",
        "lr_ovr.fit(X_train, y_train)\n",
        "\n",
        "print(\"OVR training accuracy:\", lr_ovr.score(X_train, y_train))\n",
        "print(\"OVR test accuracy    :\", lr_ovr.score(X_test, y_test))\n",
        "\n",
        "# Fit softmax classifier\n",
        "lr_mn = LogisticRegression( multi_class=\"multinomial\",   solver=\"lbfgs\") \n",
        "lr_mn.fit(X_train, y_train)\n",
        "\n",
        "print(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\n",
        "print(\"Softmax test accuracy    :\", lr_mn.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Cg7ZVJXnM7C",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing multi-class logistic regression\n",
        "In this exercise we'll continue with the two types of multi-class logistic regression, but on a toy 2D data set specifically designed to break the one-vs-rest scheme.\n",
        "\n",
        "The data set is loaded into X_train and y_train. The two logistic regression objects,lr_mn and lr_ovr, are already instantiated (with C=100), fit, and plotted.\n",
        "\n",
        "Notice that lr_ovr never predicts the dark blue class... yikes! Let's explore why this happens by plotting one of the binary classifiers that it's using behind the scenes.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a new logistic regression object (also with C=100) to be used for binary classification.\n",
        "Visualize this binary classifier with plot_classifier... does it look reasonable?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15u7hB3lnx7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZR-OaEInOkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print training accuracies\n",
        "print(\"Softmax     training accuracy:\", lr_mn.score(X_train, y_train))\n",
        "print(\"One-vs-rest training accuracy:\", lr_ovr.score(X_train, y_train))\n",
        "\n",
        "# Create the binary classifier (class 1 vs. rest)\n",
        "lr_class_1 = ____\n",
        "lr_class_1.fit(X_train, y_train==1)\n",
        "\n",
        "# Plot the binary classifier (class 1 vs. rest)\n",
        "plot_classifier(X_train, y_train==1, ____)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGFw_w1TnQEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print training accuracies\n",
        "print(\"Softmax     training accuracy:\", lr_mn.score(X_train, y_train))\n",
        "print(\"One-vs-rest training accuracy:\", lr_ovr.score(X_train, y_train))\n",
        "\n",
        "# Create the binary classifier (class 1 vs. rest)\n",
        "lr_class_1 = LogisticRegression(C=100)\n",
        "lr_class_1.fit(X_train, y_train==1)\n",
        "\n",
        "# Plot the binary classifier (class 1 vs. rest)\n",
        "plot_classifier(X_train, y_train==1, lr_class_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7HVbxHJnuPa",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the binary classifier incorrectly labels almost all points in class 1 (shown as red triangles in the final plot)! Thus, this classifier is not a very effective component of the one-vs-rest classifier. In general, though, one-vs-rest often works well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qhj8QuTn09g",
        "colab_type": "text"
      },
      "source": [
        "### One-vs-rest SVM\n",
        "As motivation for the next and final chapter on support vector machines, we'll repeat the previous exercise with a non-linear SVM. Once again, the data is loaded into X_train, y_train, X_test, and y_test .\n",
        "\n",
        "Instead of using LinearSVC, we'll now use scikit-learn's SVC object, which is a non-linear \"kernel\" SVM (much more on what this means in Chapter 4!). Again, your task is to create a plot of the binary classifier for class 1 vs. rest.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Fit an SVC called svm_class_1 to predict class 1 vs. other classes.\n",
        "Plot this classifie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7aewXB7nyid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We'll use SVC instead of LinearSVC from now on\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create/plot the binary classifier (class 1 vs. rest)\n",
        "svm_class_1 = ____\n",
        "svm_class_1.fit(____)\n",
        "plot_classifier(____)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC0o6nBjohcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We'll use SVC instead of LinearSVC from now on\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create/plot the binary classifier (class 1 vs. rest)\n",
        "svm_class_1 = SVC()\n",
        "svm_class_1.fit(X_train,y_train)\n",
        "plot_classifier(X_train,y_train,svm_class_1)\n",
        "print(\"SVM training accuracy:\", svm_class_1.score(X_train, y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4eS-64Woqhv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## ahora vs 1 clase \n",
        "# We'll use SVC instead of LinearSVC from now on\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create/plot the binary classifier (class 1 vs. rest)\n",
        "svm_class_1 = SVC()\n",
        "svm_class_1.fit(X_train,y_train==1)\n",
        "plot_classifier(X_train,y_train==1,svm_class_1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuZc7fElo33b",
        "colab_type": "text"
      },
      "source": [
        "## Support vectors\n",
        "### Effect of removing examples\n",
        "Support vectors are defined as training examples that influence the decision boundary. In this exercise, you'll observe this behavior by removing non support vectors from the training set.\n",
        "\n",
        "The wine quality dataset is already loaded into X and y (first two features only). (Note: we specify lims in plot_classifier() so that the two plots are forced to use the same axis limits and can be compared directly.)\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Train a linear SVM on the whole data set.\n",
        "Create a new data set containing only the support vectors.\n",
        "Train a new linear SVM on the smaller data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MM09Q7us8zD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train a linear SVM\n",
        "svm = SVC(kernel=\"linear\")\n",
        "svm.fit(____)\n",
        "plot_classifier(X, y, svm, lims=(11,15,0,6))\n",
        "\n",
        "# Make a new data set keeping only the support vectors\n",
        "print(\"Number of original examples\", len(X))\n",
        "print(\"Number of support vectors\", len(svm.support_))\n",
        "X_small = X[____]\n",
        "y_small = y[____]\n",
        "\n",
        "# Train a new SVM using only the support vectors\n",
        "svm_small = SVC(kernel=\"linear\")\n",
        "svm_small.fit(____)\n",
        "plot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUSgxvDns-PU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train a linear SVM\n",
        "svm = SVC(kernel=\"linear\")\n",
        "svm.fit(X,y)\n",
        "plot_classifier(X, y, svm, lims=(11,15,0,6))\n",
        "\n",
        "# Make a new data set keeping only the support vectors\n",
        "print(\"Number of original examples\", len(X))\n",
        "print(\"Number of support vectors\", len(svm.support_))\n",
        "X_small = X[svm.support_]\n",
        "y_small = y[svm.support_]\n",
        "\n",
        "# Train a new SVM using only the support vectors\n",
        "svm_small = SVC(kernel=\"linear\")\n",
        "svm_small.fit(X_small,y_small)\n",
        "plot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeExpMLat6Z-",
        "colab_type": "text"
      },
      "source": [
        "Compare the decision boundaries of the two trained models: are they the same? By the definition of support vectors, they should be!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZKDyj4Wt9k6",
        "colab_type": "text"
      },
      "source": [
        "## Kernel SVMs\n",
        "### GridSearchCV warm-up\n",
        "In the video we saw that increasing the RBF kernel hyperparameter gamma increases training accuracy. In this exercise we'll search for the gamma that maximizes cross-validation accuracy using scikit-learn's GridSearchCV. A binary version of the handwritten digits dataset, in which you're just trying to predict whether or not an image is a \"2\", is already loaded into the variables X and y.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a GridSearchCV object.\n",
        "Call the fit() method to select the best value of gamma based on cross-validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVHSyCvyvp5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate an RBF SVM\n",
        "svm = SVC()\n",
        "\n",
        "# Instantiate the GridSearchCV object and run the search\n",
        "parameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
        "searcher = GridSearchCV(svm, ____)\n",
        "____.fit(____)\n",
        "\n",
        "# Report the best parameters\n",
        "print(\"Best CV params\", searcher.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgu_XqMTvqZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate an RBF SVM\n",
        "svm = SVC()\n",
        "\n",
        "# Instantiate the GridSearchCV object and run the search\n",
        "parameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
        "searcher = GridSearchCV(svm, parameters)\n",
        "searcher.fit(X,y)\n",
        "\n",
        "# Report the best parameters\n",
        "print(\"Best CV params\", searcher.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmSfELQev4rD",
        "colab_type": "text"
      },
      "source": [
        "Larger values of gamma are better for training accuracy, but cross-validation helped us find something different (and better!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blb9hHAbv7d6",
        "colab_type": "text"
      },
      "source": [
        "### Jointly tuning gamma and C with GridSearchCV\n",
        "In the previous exercise the best value of gamma was 0.001 using the default value of C, which is 1. In this exercise you'll search for the best combination of C and gamma using GridSearchCV.\n",
        "\n",
        "As in the previous exercise, the 2-vs-not-2 digits dataset is already loaded, but this time it's split into the variables X_train, y_train, X_test, and y_test. Even though cross-validation already splits the training set into parts, it's often a good idea to hold out a separate test set to make sure the cross-validation results are sensible.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Run GridSearchCV to find the best hyperparameters using the training set.\n",
        "Print the best values of the parameters.\n",
        "Print out the accuracy on the test set, which was not used during the cross-validation procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjGfHoBpv92w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate an RBF SVM\n",
        "svm = SVC()\n",
        "\n",
        "# Instantiate the GridSearchCV object and run the search\n",
        "parameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
        "searcher = GridSearchCV(svm, ____)\n",
        "____.fit(____)\n",
        "\n",
        "# Report the best parameters and the corresponding score\n",
        "print(\"Best CV params\", searcher.best_params_)\n",
        "print(\"Best CV accuracy\", searcher.best_score_)\n",
        "\n",
        "# Report the test accuracy using these best parameters\n",
        "print(\"Test accuracy of best grid search hypers:\", searcher.score(____))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZHo6m0JwUT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate an RBF SVM\n",
        "svm = SVC()\n",
        "\n",
        "# Instantiate the GridSearchCV object and run the search\n",
        "parameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
        "searcher = GridSearchCV(svm, parameters)\n",
        "searcher.fit(X_train, y_train)\n",
        "\n",
        "# Report the best parameters and the corresponding score\n",
        "print(\"Best CV params\", searcher.best_params_)\n",
        "print(\"Best CV accuracy\", searcher.best_score_)\n",
        "\n",
        "# Report the test accuracy using these best parameters\n",
        "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8o_88ICwYhK",
        "colab_type": "text"
      },
      "source": [
        "Note that the best value of gamma, 0.0001, is different from the value of 0.001 that we got in the previous exercise, when we fixed C=1. Hyperparameters can affect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2yf-IP8wjRn",
        "colab_type": "text"
      },
      "source": [
        "### Comparing logistic regression and SVM (and beyond)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8Uyg8g_z2u7",
        "colab_type": "text"
      },
      "source": [
        "### An advantage of logistic regression\n",
        "Which of the following is an advantage of logistic regression over SVMs?\n",
        "\n",
        "Answer the question\n",
        "50 XP\n",
        "Possible Answers\n",
        "**It naturally outputs meaningful probabilities.**\n",
        "press\n",
        "1\n",
        "It can be used with kernels.\n",
        "press\n",
        "2\n",
        "It is computationally efficient with kernels.\n",
        "press\n",
        "3\n",
        "It learns sigmoidal decision boundaries.\n",
        "press\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B26N2rmXzvOQ",
        "colab_type": "text"
      },
      "source": [
        "An advantage of SVMs\n",
        "Which of the following is an advantage of SVMs over logistic regression?\n",
        "\n",
        "Answer the question\n",
        "50 XP\n",
        "Possible Answers\n",
        "They naturally outputs meaningful probabilities.\n",
        "press\n",
        "1\n",
        "They can be used with kernels.\n",
        "press\n",
        "2\n",
        "**They are computationally efficient with kernels.**\n",
        "press\n",
        "3\n",
        "They learn sigmoidal decision boundaries.\n",
        "press\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmAMldckz5ji",
        "colab_type": "text"
      },
      "source": [
        "### Using SGDClassifier\n",
        "In this final coding exercise, you'll do a hyperparameter search over the regularization type, regularization strength, and the loss (logistic regression vs. linear SVM) using SGDClassifier().\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Instantiate an SGDClassifier instance with random_state=0.\n",
        "Search over the regularization strength, the hinge vs. log losses, and L1 vs. L2 regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_IUz66Nz6dx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We set random_state=0 for reproducibility \n",
        "linear_classifier = ____(random_state=0)\n",
        "\n",
        "# Instantiate the GridSearchCV object and run the search\n",
        "parameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n",
        "             'loss':[____], 'penalty':[____]}\n",
        "searcher = GridSearchCV(linear_classifier, parameters, cv=10)\n",
        "searcher.fit(X_train, y_train)\n",
        "\n",
        "# Report the best parameters and the corresponding score\n",
        "print(\"Best CV params\", searcher.best_params_)\n",
        "print(\"Best CV accuracy\", searcher.best_score_)\n",
        "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PieWEc2h0UfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We set random_state=0 for reproducibility \n",
        "linear_classifier = SGDClassifier(random_state=0)\n",
        "\n",
        "# Instantiate the GridSearchCV object and run the search\n",
        "parameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n",
        "             'loss':['log','hinge'], 'penalty':['l1','l2']}\n",
        "searcher = GridSearchCV(linear_classifier, parameters, cv=10)\n",
        "searcher.fit(X_train, y_train)\n",
        "\n",
        "# Report the best parameters and the corresponding score\n",
        "print(\"Best CV params\", searcher.best_params_)\n",
        "print(\"Best CV accuracy\", searcher.best_score_)\n",
        "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db9kYwf406Ff",
        "colab_type": "text"
      },
      "source": [
        "One advantage of SGDClassifier is that it's very fast - this would have taken a lot longer with LogisticRegression or LinearSVC"
      ]
    }
  ]
}